#!/bin/bash

# Start script for Airflow data pipeline
set -e

echo "üöÄ Starting Apache Airflow Data Pipeline..."

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "‚ùå Docker is not running. Please start Docker first."
    exit 1
fi

# Create necessary directories
mkdir -p data/{raw,processed,logs}
mkdir -p logs

# Set environment variables
export AIRFLOW_UID=$(id -u)
export AIRFLOW_GID=0
export AIRFLOW_PROJ_DIR=$(pwd)

echo "üìã Environment Configuration:"
echo "   AIRFLOW_UID: $AIRFLOW_UID"
echo "   AIRFLOW_PROJ_DIR: $AIRFLOW_PROJ_DIR"

# Generate Fernet key if not exists
if [ -z "$AIRFLOW__CORE__FERNET_KEY" ]; then
    echo "üîë Generating Fernet key..."
    export AIRFLOW__CORE__FERNET_KEY=$(python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
fi

# Check resource requirements
echo "üîç Checking system requirements..."

MEMORY_GB=$(free -g | awk 'NR==2{printf "%.1f", $7}')
CPU_CORES=$(nproc)
DISK_GB=$(df -BG . | awk 'NR==2 {print $4}' | sed 's/G//')

echo "   Available Memory: ${MEMORY_GB}GB"
echo "   CPU Cores: ${CPU_CORES}"
echo "   Available Disk: ${DISK_GB}GB"

# Warnings for insufficient resources
if (( $(echo "$MEMORY_GB < 4" | bc -l) )); then
    echo "‚ö†Ô∏è  Warning: Less than 4GB memory available. Performance may be affected."
fi

if (( CPU_CORES < 2 )); then
    echo "‚ö†Ô∏è  Warning: Less than 2 CPU cores available. Performance may be affected."
fi

# Initialize Airflow (first time setup)
echo "üîß Initializing Airflow..."
docker-compose up airflow-init

# Start the services
echo "üåü Starting Airflow services..."
docker-compose up -d

# Wait for services to be healthy
echo "‚è≥ Waiting for services to be healthy..."
sleep 30

# Check service health
echo "üè• Checking service health..."
docker-compose ps

# Display access information
echo ""
echo "‚úÖ Airflow Data Pipeline is starting up!"
echo ""
echo "üìä Access URLs:"
echo "   Airflow UI:    http://localhost:8080"
echo "   Flower UI:     http://localhost:5555 (if enabled)"
echo "   Grafana:       http://localhost:3000"
echo "   Prometheus:    http://localhost:9090"
echo ""
echo "üîë Default Credentials:"
echo "   Airflow:  admin / admin"
echo "   Grafana:  admin / admin"
echo ""
echo "üìù Logs:"
echo "   docker-compose logs -f [service-name]"
echo "   Available services: airflow-webserver, airflow-scheduler, airflow-worker"
echo ""
echo "üõ†Ô∏è  Management Commands:"
echo "   Stop:    docker-compose down"
echo "   Restart: docker-compose restart"
echo "   Cleanup: docker-compose down -v"
echo ""
echo "üéØ Next Steps:"
echo "   1. Open Airflow UI at http://localhost:8080"
echo "   2. Enable the 'large_data_etl_pipeline' DAG"
echo "   3. Enable the 'pipeline_monitoring_system' DAG"
echo "   4. Configure email alerts in the .env file"
echo "   5. Set up your data sources"
